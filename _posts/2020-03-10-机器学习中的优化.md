---
layout:     post                    # 使用的布局（不需要改）
title:      机器学习中的优化            # 标题 
subtitle:   拉格朗日乘子法与KKT条件 #副标题
date:       2020-03-10          # 时间
author:     Alkane                      # 作者
header-img: img/post-bg-hacker.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 数值分析

---

# 机器学习中的优化

## 前言

最近在看机器学习，发现里面很多数学味很浓的模型都需要用到拉格朗日乘子法来做最优化的分析，当然这些模型都是凸的，本文主要讨论和学习一下拉格朗日乘子法和KKT条件。

文章为两部分，前一部分主要是结论性的东西，对感性理解不感兴趣的读者只看前一部分就可以了。

后半部分是自己的一些理解。

## 拉格朗日乘子法的总结

> 拉格朗日乘子法(Lagrange multipliers)是一种寻找多元函数在一组约束下的极值的方法，通过引入拉格朗日乘子，可将有$d$个变量与$k$个约束条件的最优化问题，转化为具有$d+k$个变量的无约束优化问题求解。

考虑下面这样的优化问题:


$$
\begin{align*}
\min_x \quad &f(x)\\
s.t.\quad &h_i(x)=0\quad(i=1,...,m)\\
&g_j(x)\le 0\quad(j=1,...,n)
\end{align*}
$$


其中$x\in R^d$



引入拉格朗日乘子$\lambda = (\lambda_1,...\lambda_m)^T$和$\mu=(\mu_1,...\mu_n)^T$.得到的拉格朗日函数为:


$$
L(x,\lambda,\mu) = f(x)+\sum_{i=1}^{m}\lambda_ih_i(x)+\sum_{j=1}^{n}\mu_jg_j(x).
$$


该拉格朗日函数对应的KKT条件为:


$$
\left\{\begin{array}{l}
g_j(x)\le0\\
\mu_j\ge0\\
\mu_jg_j(x)=0
\end{array}\right.
$$


上面的拉格朗日函数的对偶函数为:


$$
\Gamma(\lambda,\mu) =  \inf_{x}\quad L(x,\lambda,\mu)
$$


设$\hat{x}$为可行域中的一个点,那么对于任意的$\mu\ge 0$和$\lambda$有：


$$
\sum_{i=1}^{m}\lambda_i h_i(x) + \sum_{j=1}^{n}\mu_jg_j(x)\le 0
$$


也就是:


$$
\Gamma(\lambda,\mu) = \inf_{x}\quad L(x,\lambda,\mu) \le L(\hat{x},\lambda,\mu)\le f(\hat{x})
$$


也就是说,$\Gamma$是$f$的一个下界,如果我们能求出$\Gamma$的最大值,就可以对$f$的最小值有一个更清楚的了解，但是这里存在一个问题，$\Gamma$这个下界有多接近$f$的最小值？



我们称:


$$
\max_{\lambda,\mu} \quad \Gamma(\lambda,\mu) .\quad s.t. \mu\ge 0
$$


为原优化问题的对偶问题。



对偶问题得到的最大值$d$ ,与原问题的最小值$p$，一定有$d\le p$,这称为“**弱对偶性**”



如果$d=p$，则称为“**强对偶性**”。



在数学中，如果能保证原问题是凸优化问题，并且可行域中至少存在一点让不等式约束严格成立(取不等号),就有强对偶性成立。



这时我们只需要解决容易解决的对偶问题，就能得到原问题的解了。



## KKT 条件

KKT条件是:Karush-Kuhn-Tucker 这三位数学家的缩写，出现在凸优化问题里含有不等式约束的情况。





