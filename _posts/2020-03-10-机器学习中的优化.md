---
layout:     post                    # 使用的布局（不需要改）
title:      机器学习中的优化            # 标题 
subtitle:   拉格朗日乘子法与KKT条件 #副标题
date:       2020-03-10          # 时间
author:     Alkane                      # 作者
header-img: img/post-bg-hacker.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 数值分析

---

# 机器学习中的优化

## 前言

最近在看机器学习，发现里面很多数学味很浓的模型都需要用到拉格朗日乘子法来做最优化的分析，当然这些模型都是凸的，本文主要讨论和学习一下拉格朗日乘子法和KKT条件。

文章为两部分，前一部分主要是结论性的东西，对感性理解不感兴趣的读者只看前一部分就可以了。

后半部分是自己的一些理解。

## 拉格朗日乘子法的总结

> 拉格朗日乘子法(Lagrange multipliers)是一种寻找多元函数在一组约束下的极值的方法，通过引入拉格朗日乘子，可将有$d$个变量与$k$个约束条件的最优化问题，转化为具有$d+k$个变量的无约束优化问题求解。

考虑下面这样的优化问题:


$$
\begin{align*}
\min_x \quad &f(x)\\
s.t.\quad &h_i(x)=0\quad(i=1,...,m)\\
&g_j(x)\le 0\quad(j=1,...,n)
\end{align*}
$$


其中$x\in R^d$



引入拉格朗日乘子$\lambda = (\lambda_1,...\lambda_m)^T$和$\mu=(\mu_1,...\mu_n)^T$.得到的拉格朗日函数为:


$$
L(x,\lambda,\mu) = f(x)+\sum_{i=1}^{m}\lambda_ih_i(x)+\sum_{j=1}^{n}\mu_jg_j(x).
$$


该拉格朗日函数对应的KKT条件为:


$$
\left\{\begin{array}{l}
g_j(x)\le0\\
\mu_j\ge0\\
\mu_jg_j(x)=0
\end{array}\right.
$$


上面的拉格朗日函数的对偶函数为:


$$
\Gamma(\lambda,\mu) =  \inf_{x}\quad L(x,\lambda,\mu)
$$


设$\hat{x}$为可行域中的一个点,那么对于任意的$\mu\ge 0$和$\lambda$有：


$$
\sum_{i=1}^{m}\lambda_i h_i(x) + \sum_{j=1}^{n}\mu_jg_j(x)\le 0
$$


也就是:


$$
\Gamma(\lambda,\mu) = \inf_{x}\quad L(x,\lambda,\mu) \le L(\hat{x},\lambda,\mu)\le f(\hat{x})
$$


也就是说,$\Gamma$是$f$的一个下界,如果我们能求出$\Gamma$的最大值,就可以对$f$的最小值有一个更清楚的了解，但是这里存在一个问题，$\Gamma$这个下界有多接近$f$的最小值？



我们称:


$$
\max_{\lambda,\mu} \quad \Gamma(\lambda,\mu) .\quad s.t. \mu\ge 0
$$


为原优化问题的对偶问题。



对偶问题得到的最大值$d$ ,与原问题的最小值$p$，一定有$d\le p$,这称为“**弱对偶性**”



如果$d=p$，则称为“**强对偶性**”。



在数学中，如果能保证原问题是凸优化问题，并且可行域中至少存在一点让不等式约束严格成立(取不等号),就有强对偶性成立。



这时我们只需要解决容易解决的对偶问题，就能得到原问题的解了。



## KKT 条件

KKT条件是:Karush-Kuhn-Tucker 这三位数学家的缩写，出现在凸优化问题里含有不等式约束的情况，在上面已经具体讲解过了。



## 理解

拉格朗日乘子法求解的问题是最优化的问题，但是一般都会有约束，约束有两种:

- 等式约束，如 $h(x)=0$.
- 不等式约束,如 $ g(x)\le 0$.

其中的等式约束就相当于把可行解的空间约束在曲面$h(x)=0$上,而不等式约束就相当于把可行解约束在$ g(x)\le 0$内部.

如果函数$f(x)$得到了最优解，我们可以得到这样的结论:

- 对于**只有等式约束的**情况：最优点$x^*$上$\nabla f(x^*)$正交于曲面$h(x) = 0$

  这是显然的，因为如果$f(x)$的梯度与$h(x)=0$是不正交的，那么沿着负梯度方向在x所在处的切线方向的分量走，总是可以让$f(x)$变得更小,所以在最优点处的梯度方向一定是正交于约束曲面$h(x) = 0$.

  换个角度也就是:

  

  $$
  \nabla f(x^*)+\lambda \nabla h(x^*)=0
  $$

  

  这是显然的，因为梯度方向正交于约束曲面，则$f$的梯度方向于$h$的梯度方向平行。

- 对有**不等式约束的情况**:拉格朗日函数$ L(x,\lambda)=f(x)+\lambda g(x)$

  我们记$g(x) \le 0$这个可行区域记作$K$

  - $g(x^*)<0$ : 最佳解在$K$的内部，此时约束条件其实是没有用上的,约束优化问题退化成无约束问题,只需要点满足$\nabla f(x)=0,\lambda =0$就可以得到最优解
  - $g(x^*)=0$ : 最佳解在$K$的边界, 约束变成$ g(x) = 0$,和上面提到的等式约束情况一致,有$\nabla f = -\lambda \nabla g$,这里的$\lambda$应该大于等于0，因为我们希望$f$的梯度方向是指向$K$的内部,这样可以保证得到的解是最小解(如果求最大解应该反过来,但是最小解和最大解本质相同)。

  因此,不论那种情况，我们都有$\lambda g(x)=0$,这个条件叫做**互补松弛性**,整理上述情况可以有KKT条件:

  
  $$
  \left\{\begin{array}{l}
  g_j(x)\le0\\
  \mu_j\ge0\\
  \mu_jg_j(x)=0
  \end{array}\right.
  $$
  

## 参考
《机器学习》- 周志华

  

