---
layout:     post                    # 使用的布局（不需要改）
title:      人工智能导论笔记【马尔可夫决策过程】              # 标题 
subtitle:   更贴近真实决策的概率模型 #副标题
date:       2020-03-26          # 时间
author:     Alkane                      # 作者
header-img: img/ai_2020.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 人工智能

---

# 马尔可夫决策过程

前面我们介绍了搜索的一些方法和思想，这些方法都隐含着一个前提，在某一个状态做同样的行为可以导致相同的结果。

但是事实上，在现实生活中这常常是达不到的条件，我们经常会说: 啊！这次运气不好，再来再来。也就是说，在更普遍的情形里，常常认为从一个状态到另一个状态是有概率成功，有概率到其他状态的。

这也就是马尔可夫过程的基本假设。下面给出定义一个马尔可夫决策过程需要的几条性质。

- 一个状态空间$S$,这里的状态与此前的搜索时提到的状态空间是一样的。
- 一个动作集合$A$,这里的动作与此前的搜索时提到的动作集合也是一样的。
- 一个开始状态
- 一个或多个结束状态
- (可能)一个衰减因子$\gamma$
- 一个状态转移函数$T(s,a,s’)$,这说明了给定状态$s\in S$,$a\in A$,得到状态$s’\in S$的概率
- 一个反馈函数$R(s,a,s’)$,我们需要一个反馈函数来指导我们做决策，如果做动作没有反馈，那还有什么决策的必要呢 : ) ?



## 简单说明一下马尔可夫过程

马尔可夫过程中,t+1时刻的状态$S_{t+1}$等于$s’$的概率只取决于上一个状态$S_t$和其采取的动作$A_t$，也就是说:


$$
P(S_{t+1}=s'|S_t=s_t,A_t=a_t,S_{t-1}=s_{t-1},A_{t-1},\dots,S_0=s_0)\\
= P(S_{t+1}=s'|S_t=s_t,A_t=a_t)
$$


这就说明了马尔可夫过程做决策时，只需要考虑当前状态和即将采取的动作，这当然也是一个简化的模型，但是我们需要的不是最准确的模型，我们只需要够准确的模型并且足够好用就行了:).

这样的性质就可以保证我们的状态转移函数$T(s,a,s’)$等于$P(S_{t+1}=s’|S_t=s_t,A_t=a_t)$.

## 一个例子

一个汽车在行驶过程中有三种状态,*cool,warm,overheated*.

并且它有两种动作可以选择:*Slow,Fast*.

下图的状态转移图就能说明采取不同的动作可能得到的结果和获得的反馈。



![](https://pic.downk.cc/item/5e7ea3c5504f4bcb04fb2f55.jpg)

下面我们用搜索树的结构来表示一次可能的状态转移(深度为2的树):

![](https://pic.downk.cc/item/5e7ea956504f4bcb04fea475.jpg)



## 效用

说到决策过程的效用，也就是从开始状态到结束状态时的反馈的某种度量。说白了就是衡量我们的决策过程是好还是不好。

首先我们会想到这样的效用表示:


$$
U[s_0,a_0,s_1,a_1,\dots,] = R(s_0,a_0,s_1)+R(s_1,a_1,s_2)+\dots
$$


也就是简单的把每次决策的反馈做算术相加，这样的做法会有什么样的问题呢？



看我们上面的小汽车的例子，就会发现，每次都选择Slow这个动作不仅有正反馈，而且可以一直做下去，趋于无穷。这样的做法显然不是最优的，但是从我们对决策的衡量效用$U$来看,却是一个很不错的选择。

怎样避免出现这样的问题呢？我们可以给每次的回馈乘上一个衰减因子$\gamma^t$,这样可以保证在足够多次的状态转移后的回馈几乎为0，让决策更专注于眼前的得失。

也就是这样的效用表示:
$$
U[s_0,a_0,s_1,a_1,\dots,] = \gamma ^0R(s_0,a_0,s_1)+\gamma^1R(s_1,a_1,s_2)+\dots
$$
并且我们还可以保证，当$\gamma$满足$|\gamma|<1$的条件时,这样的效用一定是收敛的。

## 定义马尔可夫决策过程

给定一个马尔可夫决策过程需要的状态空间$S$,动作集合$A$，等等条件，我们希望做到的就是找到一组策略$\pi^*:S\rightarrow A$,对于每一个给定的状态$s\in S$,都可以通过$\pi^*(s)\in A$,来更新状态，并且达到效用最大值。

显然这样的最优策略是与初始状态$s_0$以及状态转移函数有关的，采取不同的衰减因子也会对结果有影响。

## 贝尔曼方程

说清楚上面的马尔可夫决策是什么东西以后，我们还需要做的事情就是如何找到这样的最优策略$\pi^*$.

首先我们来定义两个函数。

- $V^*(s)$,状态$s$的最佳值:从状态$s$开始往后直到结束,每次都选取最优解,所预期的效用。
- $Q^*(s,a)$,状态$s$处采取动作a的品质(Quality),在状态$s$采取了动作$a$后,每次都选取最优解，往后直到结束预期的效用。

这样我们就能写出下面的式子:


$$
\begin{align*}
V^*(s) &= \max_{a\in A}Q^*(s,a)\\
Q^*(s,a)&=\sum_{s'}T(s,a,s')[R(s,a,s')+\gamma V^*(s')]\\
V^*(s)&=\max_{a}\sum_{s'}T(s,a,s')[R(s,a,s')+\gamma V^*(s')]
\end{align*}
$$


一般我们把第三个式子称作贝尔曼方程(Bellman Equation).

在求解这个方程的时候有些需要注意的地方:

- 方程是递推形式的
- 如果状态可以一直转换下去,搜索树将会爆炸，从而问题无法解决

为了解决搜索树爆炸的问题,我们这里需要限制搜索树的深度。

如果要把搜索树的深度限制在k层,就可以得到:

$V_k(s)$: 当状态转移k次后进入结束状态时的$V^*(s)$.

下面是用迭代方法得到的求解算法:

### Value Iteration

> - Step 1: $\forall s\in S$ ,初始化 $V_0(s)=0$.
> - Step 2: 重复下面步骤直到$V_k(s)$收敛:
>   - $\forall s \in S$,$V_{k+1}(s)=\max_a\sum_{s'}T(s,a,s')[R(s,a,s')+\gamma V_{k}(s')]$

什么时候我们可以认为$V_k(s)$收敛呢?,其实只要我们算出来$\forall s \in S,V_k(s)=V_{k+1}(s)$,就可以说明已经收敛了。

那么这个算法一定会收敛吗？下面给出一个简单的证明:

> Proof: $V_k$ will converge.
>
> - Case 1: 如果搜索树会有最大深度的话,按照我们的算法，一定会到达这个最大深度，或者在这之前就已经收敛。
>
> - Case 2: 如果衰减因子$\gamma$小于1.
>
>   - 我们把$V_k(s),V_{k+1}(s)$这两棵搜索树画出来,为了保持深度一致,在$V_k(s)$的最底层填充0.
>
>   ![](https://pic.downk.cc/item/5e7efa0b504f4bcb043ee397.jpg)
>
>   - 显然$V_{k+1}(s)$的最底层的大小居于$(R_{min},R_{max})$之间,而这样的话,由于层数每增加一层,都要乘一项衰减因子$\gamma$,我们可以得到$V_k(s),V_{k+1}(s)$之间的差距最大不过$\gamma^k |R_{max}|$,而$\gamma$又小于1,当k越来越大时,这样的差距会越来越小(指数速度的收敛),所以算法会收敛。



