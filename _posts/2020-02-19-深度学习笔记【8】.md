---
layout:     post                    # 使用的布局（不需要改）
title:      深度学习笔记【8】              # 标题 
subtitle:   目标检测之边框回归 #副标题
date:       2020-02-19          # 时间
author:     Alkane                      # 作者
header-img: img/post-bg-hacker.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 目标检测

---



# Eight

目标检测是计算机视觉领域中的常见任务,而目标检测的一种常见的需求就是将我们需要检测的物体在图中**标**出来。



下图是目标检测的一种基于region proposal的方法R-CNN的大概流程



![](https://d2l.ai/_images/fast-rcnn.svg)



可以看到,在R-CNN的流程中,最上层有一个步骤就是我们具体的主题，边框回归。



## 为什么需要边框回归

首先要大概提一下R-CNN的思想,R-CNN首先要对给定的图像做一个简单的划分，比如给出1000个区域，这些区域里有可能有我们需要检测的物体，之后我们就可以运用训练好的分类器，判断这个区域里是否有我们感兴趣的物体。如果有，再对这个区域的边框做适当的调整，最后把它呈现出来。



如果不做边框回归这一步，直接将先前划分的区域给框出来就会出现这样的情况:



![](https://pic.downk.cc/item/5e4cec2248b86553ee915a38.jpg)



红色的框是我们的模型得到的，而绿色的框是实际情况。



也就是说，我们的模型得到的边框距离实际情况有偏差，这样的偏差在更精细的目标检测(如检测瞳孔)任务中是无法容忍的。



## 怎样做边框回归

我们的目标是:**尽可能地得到与实际的边框相接近的边框**



在目标检测中,常用$(x,y,w,h)$来表示一个框。



其中,$x,y$表示左上角的点的坐标,$w,h$表示边框的宽度和高度。



我们用$(P_x,P_y,P_w,P_h)$来表示模型从proposal中得到的边框$P$(也就是我们第一步简单划分时得到的边框)。



用$(\hat{G}_x,\hat{G}_y,\hat{G}_w,\hat{G}_h)$来表示边框回归后的边框$\hat{G}$.



用$(G_x,G_y,G_w,G_h)$来表示真实的边框$G$.



怎样从$P$得到$G$呢？



我们知道可以通过简单的仿射变换来变换边框,也就是说:


$$
\hat{G}_x:= P_x + \delta x\\
\hat{G}_y:=P_y+\delta y\\
\hat{G}_w:=P_w\times \delta w\\
\hat{G}_h:=P_h\times \delta h
$$


考虑到不同的尺度的区域应该变化的尺度是不一样的，


$$
\begin{aligned}
&\delta x =P_w dx(P)\\
&\delta y=P_h dy(P)\\
&\delta w=\delta w(P)\\
&\delta h=\delta h(P)
\end{aligned}
$$


这四个量都应该是$P$的函数。



为了保证放缩的尺度大于0，我们可以规定:


$$
\delta w(P) = exp(dw(P))\\
\delta h(P) = exp(dh(P))
$$


用一个指数函数来保证尺度永远大于0,并且为了符号的统一，还把指数函数内部的函数写做$dw,dh$ .



这样可以把模型用到的参数写成:$d(P)$这是一个四维的向量。



最后我们实际的尺度参数也就是:


$$
\begin{aligned}
&t_{x}=\frac{G_{x}-P_{x}}{P_{w}}\\
&t_{y}=\frac{G_{y}-P_{y}}{P_{h}}\\
&t_{w}=\log \left(\frac{G_{w}}{P_{w}}\right)\\
&t_{h}=\log \left(\frac{G_{h}}{P_{h}}\right)
\end{aligned}
$$




下面我们就是要让模型学到怎样去做尺度变换，在R-CNN中，我们用CNN来提取图像的特征向量，记作$\phi(P)$,用 $W_*$ 来表示需要学习的参数,其中*表示$x,y,w,h$.


$$
d_*(P) = W_*^T\phi(P)
$$


我们需要训练的就是$W_*$，L-2的loss有:


$$
W_{*}=\operatorname{argmin}_{\bar{W}_{*}} \sum_{i}^{N}\left(t_{*}^{i}-\bar{W}_{*}^{T} \phi\left(P^{i}\right)\right)^{2}+\lambda\left\|\bar{W}_{*}\right\|^{2}
$$


用梯度下降就可以得到很好的结果。



## 参考

[caffe社区问答](http://caffecn.cn/?/question/160)