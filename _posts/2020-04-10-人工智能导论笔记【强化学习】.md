---
layout:     post                    # 使用的布局（不需要改）
title:      人工智能导论笔记【强化学习】              # 标题 
subtitle:   Reinforce Learning #副标题
date:       2020-04-10        # 时间
author:     Alkane                      # 作者
header-img: img/ai_2020.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 人工智能
---



# Reinforce Learning

之前我们已经介绍了马尔可夫决策过程,其中包括两个很重要的方法:

- Value Iteration
- Policy Iteration

这两个方法专注于从给定的马尔可夫过程的贝尔曼方程中获取解决问题的线索。

其中Value Iteration侧重于状态的价值,而Policy Iteration则更加看重应该采取什么样的措施。

这样的方法适用于**offline planning**，也就是可以想象成我们的agent对于状态转移函数与反馈函数都有full knowledge,所有的动作会得到什么样的回馈是**事先可以遇见**的，我们不需要实际去做就能够在脑海中得到关于现实世界的反馈,并且我们认为实际上的反馈就是我们事前预计的那样。

现在我们就要从**offline**走向**online**,因为现实世界是复杂的，我们不能事事都做好规划才去选取最优解，世界需要勇敢的人去探索。

## What is Reinforce Learning

我们做出下面的几点假设:

- agent没有任何关于状态转移函数与反馈函数的先验知识(这是一个纯粹的小白agent)
- agent必须要采取实际的action才能获得状态转移与反馈,通过这样的方式来探索世界并且学习具体的策略

下图展现了强化学习的核心过程,agent通过不断地action,来与环境交互,从而获得reward，并且不断更新自己的策略的过程。

![](https://pic.downk.cc/item/5e906389504f4bcb04a710b9.jpg)

在强化学习中我们面临着一个摆在眼前很实际的问题:是不断地探索世界来获取更多的了解(**exploration**);还是简单的探索几次后就不断地选取**目前看上去很好的解**(**exploitation**)?

这是我们必须要面对并且做出权衡与取舍的问题,如果探索(exploration)少了,很可能对于环境的认识是错误的,从而对于将来的选择有不利影响。如果专注于探索,而忽视利用当前的解(exploitation),则也不能很好的得到最优解,因为我们的现实世界的一切都可以看成概率分布的问题，我们永远不能说对于环境的认识是没有任何错误的，也就是探索是没有边界的，我们需要即使的将探索得到的结果消化。

强化学习主要有两种可能的方式:

- 基于模型的学习(model-based learning)
- 不基于模型的学习(model-free learning)

这里的区别在于有没有具体的状态转移函数与反馈函数。

本文接下来会对这两者做一个简单的探讨,接着我们会讨论如何尽可能平衡exploration与exploitation之间的关系。

## Model-Based Learning

在基于模型的学习里,一个agent会根据当前采取了动作$q(s,a)$后到达状态$s'$的次数来估计$\hat{T}(s,a,s')$,其实这也就相当于用**频率**来近似**概率**了。

由大数定律可以保证当试验的次数足够多时,我们有充分的把握认为得到了关于现实世界的不错的近似。

我们回顾一下之前在马尔可夫决策过程中提到的一些数学模型和术语。

- 状态空间$S$,动作空间$A$

- 策略$\pi:S\rightarrow A$,给定状态s,得到应该选取的动作a
- 状态值函数$V:S\times A\rightarrow R$,也就是$V(s,a)$表示从状态s出发,采取动作a得到的状态值,特别地$V^{\pi}(s)$表示采取策略$\pi$,从状态x出发可以得到的累计回馈
- 状态-动作值函数$Q:S\times A\rightarrow R$,其中$Q^{\pi}(s,a)$表示从状态s出发,采取动作a后选用策略$\pi$可以得到的累计回馈。

如果用公式来表示状态值函数的话,如下:


$$
\begin{align*}
V^*(s) &= \max_{a\in A}Q^*(s,a)\\
Q^*(s,a)&=\sum_{s'}T(s,a,s')[R(s,a,s')+\gamma V^*(s')]\\
V^*(s)&=\max_{a}\sum_{s'}T(s,a,s')[R(s,a,s')+\gamma V^*(s')]
\end{align*}
$$
上面的$V^*,Q^*$都是$V^{\pi^*},Q^{\pi^*}$的简写,$\pi^*$表示最优策略。

之前提到过在具体决策中我们更关注在每一个状态处应该采取的动作的效益,因为这对我们做出决策是十分有帮助的。

从上面的式子里不难把状态-动作值函数写成:


$$
Q^{*}(s,a) = \sum_{s'}T(s,a,s')[R(s,a,s')+\gamma \max_{a'\in A}Q^*(s',a')]
$$


这个等式也叫做最优贝尔曼等式,其唯一解是最优状态-动作值函数

这个等式揭示了非最优策略该如何改进:将策略选择的动作变成当前最优的动作。

不妨令动作改变后对应的策略为$\pi'$,改变动作的条件为:$Q^{\pi}(x,\pi'(x))\ge V^{\pi}(x)$.

也即是采取了新的策略后,如果状态-动作值变大了,并且比当前的状态值还要大,我们就有充分的理由更新在这一个状态处的策略，理由如下:


$$
\begin{align*}
V^{\pi}(x)&\le Q^{\pi}(x,\pi'(x))\\
&=\sum_{x'}T(x,\pi'(x),x')[R(x,\pi'(x),x')+\gamma V^{\pi}(x')]\\
&\le\sum_{x'}T(x,\pi'(x),x')[R(x,\pi'(x),x')+\gamma Q^{\pi}(x',\pi(x'))]\\
&=V^{\pi'}(x)
\end{align*}
$$


这说明改进后的策略比之前的策略是单调递增的。

此时我们让$\pi'(x)=\arg\max_{a\in A}Q^{\pi}(x,a)$.

知道策略不再发生改变,此时最优贝尔曼等式自动满足,我们就得到了最优解。

我们再简单的复习一下之前提到的策略迭代算法(Policy Iteration),从一个初始策略出发,首先对策略进行评估,接着改进策略，如此迭代，直到策略收敛不再改变。

直接拷贝一下之前写过的算法流程:

> - Step 1:随便定一个策略$\pi_0$
>
> - Step 2:重复下面的步骤直到收敛
>
>   - 计算策略$\pi_i$的每个状态的值直到收敛(或者计算固定次数,注意这里的动作是固定的,所以不需要对每个动作做搜索,节省了很多时间)
>
>   $$
>   V^{\pi_i}_{k+1}(s) := \sum_{s'}T(s,\pi(s),s')[R(s,\pi(s),s')+\gamma V^{\pi}_{k}(s')]
>   $$
>
>   - 评价策略$\pi_i$ 
>
>   $$
>   V^{\pi_i}(s) = \sum_{s'}T(s,\pi_i(s),s')[R(s,\pi(s),s')+\gamma V^{\pi_i}(s')]
>   $$
>
>   - 改进策略$\pi_i$ 
>
>   $$
>   \pi_{i+1}(s) = \arg\max_{a}\sum_{s'}T(s,a,s')[R(s,a,s')+\gamma V^{\pi_i}(s')]
>   $$

好了,我们已经将基于模型的强化学习的主要思想都介绍完毕,可以看出在模型已知的情况下,强化学习的过程可以归结为动态规划的寻优问题。



## Model-Free Learning

下图是一个关于有模型和没有模型学习的区别:)

![](https://pic.downk.cc/item/5e9131b7504f4bcb044ed6e5.jpg)

通俗的讲,有模型的学习相当于是先确立大致的模型框架,在实践中把框架丰满起来，之后用这个框架来指导实践。

没有模型的学习就是直接在实践中学习,本身虽然也学到了所谓的框架，但是脑子里没有框架的意识。

### Direct Evaluation

首先我们要介绍的方法叫做直接评估,这个方法的思想十分简单:

- 固定策略$\pi$,让agent在策略$\pi$的指导下参与实践
- 当agent在实践的同时,agent会记录经过每一个状态得到的效用,与经过该状态的次数，做平均，把这个平均值当作状态s的状态值函数

下面是直接评估法的一个例子:

![](https://pic.downk.cc/item/5e91636a504f4bcb047aaa0a.jpg)

这里有一点需要特别解释的,可以看得出,每次agent经过C都会得到负的reward,那为什么最后得到的结果中状态C的状态值反而是正的呢?

这是因为在我们计算到达该状态得到的回馈时,总是要把眼光放长远一点,也就是要看到到达该状态后,agent还会转移到其他状态,要把在那之后的reward一并加起来。

我们就以C为例,来计算一下:

- Episode 1: 状态C的效用: -1 + 10 = 9
- Episode 2: 状态C的效用: -1 + 10 = 9
- Episode 3: 状态C的效用: -1 + 10 = 9
- Episode 4: 状态C的效用: -1 + -10 = -11
- 总计: 9+9+9+-11 = 16, 考虑经过C共4次,平均后得到C的状态值为: 4

是不是十分简单?这也是直接评估法的最大优点。

我们总结一下直接评估法的优点:

- 简单
- 不需要状态转移概率T和回馈函数R的任何先验知识
- 只要重复的次数足够多,我们可以得到几乎正确的结果

有一定阅历的朋友一定知道,世界上没有完美的事物,**成年人的世界不容易**.

直接评估法也有它的缺点:

- 它不会考虑相邻状态之间的转移关系(浪费了这方面的信息,当然这会影响算法的效率)
- 每一个状态的状态值都是单独学到的(没有考虑状态之间的联系,这不辩证唯物主义)
- 算法需要的时间很长
- 事实上，在现实世界里,很多动作做完了是不可逆的或者是我们无法承担的,比如让机器人从高楼跳下去:)

为了解决直接评估法中的一些直接缺陷，我们要从其缺点入手,比如我们能不能考虑相邻状态之间的转移关系呢？

如果可以借助贝尔曼方程,我们就可以轻松做到这一点,比如：

如果我们处于状态$s$,借助策略$\pi$,选择动作$\pi(s)$后转移到$s'$，我们可以用简化了的贝尔曼方程来描述这个行为:


$$
\begin{array}{l}V_{0}^{\pi}(s)=0 \\ V_{k+1}^{\pi}(s) \leftarrow \sum_{s^{\prime}} T\left(s, \pi(s), s^{\prime}\right)\left[R\left(s, \pi(s), s^{\prime}\right)+\gamma V_{k}^{\pi}\left(s^{\prime}\right)\right]\end{array}
$$


![](https://pic.downk.cc/item/5e91679e504f4bcb047e9c29.jpg)

显然这样的算法能够充分利用状态之间的关系,可是……

不是说好了不知道T和R了吗QAQ.

当然我们总能想到解决的方法,上面的依概率求和可以被我们用下面的手段来做近似:


$$
\begin{aligned} \text {sample}_{1} &=R\left(s, \pi(s), s_{1}^{\prime}\right)+\gamma V_{k}^{\pi}\left(s_{1}^{\prime}\right) \\ \text {sample}_{2} &=R\left(s, \pi(s), s_{2}^{\prime}\right)+\gamma V_{k}^{\pi}\left(s_{2}^{\prime}\right) \\ \ldots &\\ \text {sample}_{n} &=R\left(s, \pi(s), s_{n}^{\prime}\right)+\gamma V_{k}^{\pi}\left(s_{n}^{\prime}\right) \\ V_{k+1}^{\pi}(s) & \leftarrow \frac{1}{n} \sum_{i} \text {sample}_{i} \end{aligned}
$$
当然更具体的内容在下面:)

### Temporal Difference Learning

时序差分学习的思想是:

- **从每一次经验中学习**,而不是简单的记录总效用和访问状态的次数
- 我们使用**固定策略**产生的方程系统和贝尔曼方程来确定在该策略下的状态值

而算法的几个主要部分分别是:

- 状态值函数采样(sample)
  $$
  sample = R(s,\pi(s),s') +\gamma V^{\pi}(s')
  $$

- 状态值函数的更新(update)
  $$
  V^{\pi}(s) = (1-\alpha)V^{\pi}(s)+\alpha(sample)
  $$
  

之所以要乘上一个因子是为了减小之前sample的影响,要给最新的sample一个更大的权重(毕竟刚开始学习的时候采取的策略是无脑的,不应该和经过学习后得到的策略有相同的比重)

这样我们就解决了不知道状态转移概率T和反馈函数R的问题了。

当我们的状态值函数收敛以后,就可以用之前马尔可夫决策过程中的策略提取来获取在该状态值下的最优策略了。

停停停,这里有问题,我们的状态值函数是在固定策略下学到的,也就是如果一开始的策略是不一样的,我们的状态值函数应该也会不一样吧？这样不能保证最后的策略最优啊？

是的,不能保证,事实上直接评估法也有类似的问题。

这里的问题出在了我们学习的目标是状态值,如果我们学习的目标是状态-动作值呢？

这样似乎就不会出现上面这样的问题了。

这也是我们接下来要介绍的Q-learning的目标。

### Q-Learning

如果我们先把T,R不可知的问题放在一边,我们很快能写出这样的式子:


$$
Q_{k+1}(s, a) \leftarrow \sum_{s^{\prime}} T\left(s, a, s^{\prime}\right)\left[R\left(s, a, s^{\prime}\right)+\gamma \max _{a^{\prime}} Q_{k}\left(s^{\prime}, a^{\prime}\right)\right]
$$


这事实上就是贝尔曼方程在Q-Value Iteration中的式子。

相信你从上面两个方法的介绍中已经知道我们想要做什么了,用sample来近似替代这个式子中的某些部分,然后迭代出结果。

是的,我们正要这么做。

- 采样
  $$
  sample = R(s,a,s')+\gamma\max_{a'}Q(s',a')
  $$
  

- 更新
  $$
  Q(s,a)\leftarrow (1-\alpha)Q(s,a)+\alpha(sample)
  $$



可以证明,当Q收敛时,我们就得到了最优策略。这与你之前采取的策略是否最优无关:)

这一点是由式子中的max来保证的,另外需要说明的是,我们采样只获得reward的近似,也就是上面式子的sample其实可以写成这样:


$$
sample = reward + \gamma\max_{a'}Q(s',a')
$$


也就是虽然我们写成sample是两项之和,但是只有第一部分是真正sample得到的，后面一部分是根据已经获得的Q的值来计算出的。

最优性质还需要下面几点来保证:

- 最初采取的策略要能遍历所有的状态充分多次(这点不难做到,瞎走呗)
- decay因子$\alpha$要之间减小,随着时间的增加,新的sample不应该让原来的函数出现太大的变化，这是保证收敛必须的，但是也不能减小的太快，这也是一个调参的问题:)

## Exploration-Exploitation dilemma

我们在一开始就提到了关于探索(exploration)与利用(exploitation)之前的两难问题。

一个简单的想法是,每次做决定之前丢一个骰子,让它来决定我们是探索(也就是随便选取动作瞎走),还是利用(选取当前状态-动作值最大的动作).

这样的方法叫做$\mathcal{E}$贪心法

### $\mathcal{E}-Greedy$

在面临状态选择时,以概率$\mathcal{E}$来选择探索,$1-\mathcal{E}$的概率来利用。

显然,这样的方案的参数必须要随时间递减,因为我们的策略必然是越来越好的,不能总采用随机的方式来探索世界。

另外需要说明的,这里的探索不是只去没有去过的状态,而是随机选取状态的意思,不会因为状态之间的区别而区别对待。

$\mathcal{E}-Greedy$这样的方法还是过于简单了,如果我们想要更加有效的方法呢？

### Exploration Function

我们可以为每一个状态安排一个bounces，给它一个属于未曾探索的奖励，这样就可以把Q-Learning和其结合起来。

首先我们定义的函数长这样:


$$
f(s, a)=Q(s, a)+\frac{k}{N(s, a)}
$$


可以看出,除了状态-动作值函数以外,还多了一个与当前状态-动作访问次数的函数，这会随着访问次数的增大而不断减小。

也就是给较少访问的状态-动作一个比较大的奖励。

如果这样定义了，我们的Q-Learning中的Q，就可以用f来替换,不影响Q-Learning的最优性。



## Approximate Learning

最后我来简单说一下近似学习。

显然我们不可能在真实世界中使用上述方法,上述方法都要求我们记住探索过的状态,而对于相似的状态,如果之前没有探索过是识别不出来的.

这就和机器学习里的问题相似了,我们当然可以记住每一笔训练资料,但是如果遇到没有见过的新的资料,就没法解决问题了，这涉及到所谓的泛化问题。

与机器学习类似的方法，我们把状态用特征函数来表示，可以看成对状态进行一个哈希编码，把相似的状态映射置相同的函数值上,着有利于我们的算法处理复杂问题。

至于如何训练这样的新模型,也是类似的，不过这里就不啰嗦了。

## Reference

[BAIR](https://inst.eecs.berkeley.edu/~cs188/sp20/)