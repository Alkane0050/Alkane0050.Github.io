---
layout:     post                    # 使用的布局（不需要改）
title:      深度学习笔记【5】               # 标题 
subtitle:    神经网络训练算法【SGDM】#副标题
date:       2019-07-25            # 时间
author:     Alkane                      # 作者
header-img: img/post-bg-hill.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 神经网络
---

# Five

熟悉机器学习的人肯定都听说过**SGD**(Stochastic Gradient Descent)的大名,也就是说，每一次更新参数，都选择的是当前位置的梯度方向的反方向。简单的用代码表示可以写成：

```python
x+= -learning_rate*dx
```

其中的$dx$的方向，就是当前位置的梯度方向。

## SGD的不足

但是，当我们训练的神经网络的维度变得很高的时候，**SGD**往往会经过一段十分曲折的山路才有可能到达比较好的低点，甚至有些时候，停在稍微平坦一些的半山腰就不走了。



![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/40/Saddle_point.png/220px-Saddle_point.png)



上图展示的是一个鞍点的例子，可以看到，在鞍点附近的梯度为0，但是鞍点处并不是最小值，也不是最大值。



而我们的SGD,因为需要梯度来更新方向，所以如果遇到一个梯度为0，或者说梯度十分接近0的位置，它就会天真的认为这就是我们需要的好位置，乖乖的不动弹了。



## 物理学上的启示—Momentum

