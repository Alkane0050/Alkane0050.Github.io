---
layout:     post                    # 使用的布局（不需要改）
title:      主成分分析            # 标题 
subtitle:   一种不依赖标签的特征提取方法 #副标题
date:       2020-03-20          # 时间
author:     Alkane                      # 作者
header-img: img/post-bg-hacker.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 机器学习

---

# 主成分分析(PCA)

在我们处理高维数据的时候,常常由于维数过高而无法准确的发现数据的分布特点,当我们需要将数据从高维空间映射到低维空间时,就需要使用到特征提取的方法。

![](https://pic.downk.cc/item/5e9ab19ac2a9a83be5a28c4d.jpg)

## 从零维子空间开始

首先考虑一个极端简单的情况,如果数据原本就是一维的(也就是一个点),我们想要用零维的空间来表示它怎么办?
$$
X = {\{\bold{x_1},\bold{x_2},\dots,\bold{x_N}\}}
$$
最简单的可能是:$\bold{x_1}=\bold{x_2}=\dots=\bold{x_N}$.也就是我们可以选取任何一个点来表示$X$而不用担心丢失信息.

当存在噪声的时候,如何寻找最好的零维表示呢？

也就是我们希望可以找到一个向量$\bold{m}^*$,可以使得:
$$
\bold{m}^* = \arg\min_m\frac{1}{N}\Vert x_i-\bold{m}\Vert^2
$$
如果让你随便猜一个值你会怎么做？

选平均！
$$
\bold{m}^* = \frac{1}{N}\sum \bold{x}_i=\bar{\bold{x}}
$$
这的确是正确的,可以通过求偏导的方式来证明,这里省略过程。

### 一维子空间的情况？

如果我们想进一步,想要用一个一维子空间来表示$X$,该怎么做呢？

显然,我们的一维空间可以这样描述:

$a\in \mathcal{R},x_0\in\mathcal{R}^D,\omega\in \mathcal{R}^D$,使得一维子空间中的任意元素$x$都可以被表示为:
$$
x = x_0+a\omega
$$
有了上面关于零维子空间的叙述,我们可以令:
$$
x_0 = \bar{x}
$$
如何来衡量我们选取的$a,\omega$的好坏呢？

对于$x_i\in X$,我们有:
$$
x_i\approx \bar{x}+a_i\omega
$$
我们知道这样的表示和真实值之间会有误差,称$x_i-(\bar{x}+a_i\omega)$为残差。

我们记$\bold{a}=(a_1,a_2,\dots,a_N)^T$

定义目标函数$J(\omega,\bold{a})$为:
$$
\begin{align*}
J(\omega,\bold{a})&=\frac{1}{N}\sum_{i=1}^{N}\Vert x_i-(\bar{x}+a_i\omega)\Vert ^2
\\&=\frac{1}{N}\sum_{i=1}^{N}\Vert a_i\omega-(x_i-\bar{x})\Vert^2
\\&=\sum_{i=1}^{N}\frac{a_i^2\Vert \omega\Vert^2+\Vert x_i-\bar{x}\Vert^2-2a_i\omega^T(x_i-\bar{x})}{N}
\end{align*}
$$
我们希望最小化这个函数。

对其求偏导有:
$$
\begin{align*}
\frac{\partial{J}}{\partial{a_i}} &= \frac{2}{N}(a_i\Vert\omega\Vert^2-\omega^T(x_i-\bar{x}))=0\\
\frac{\partial{J}}{\partial{\omega}} &= \frac{2}{N}\sum_{i=1}^{N}(a_i^2\omega-a_i(x_i-\bar{x}))=\bold{0}\\

\end{align*}
$$
从而可以得到:
$$
a_i = \frac{(x_i-\bar{x})^T\omega}{\Vert \omega\Vert^2}
$$
由此得到:
$$
a_i\omega =\frac{(x_i-\bar{x})^T\omega}{\Vert \omega\Vert^2}\omega=\frac{c\omega(x_i-\bar{x})^Tc\omega^T}{\Vert c\omega\Vert^2}
$$
也就是$\omega$的长度对最优解的$a_i\omega$没有影响,为了简化问题我们令$\Vert \omega\Vert=1$,这并不会造成解的变化。

现在我们有:
$$
a_i = (x_i-\bar{x})^T\omega
$$
重新带回$J$后有:
$$
J(\omega,\bold{a})=\frac{1}{N}\sum_{i=1}^{N}[\Vert x_i-\bar{x}\Vert^2-a_i^2]
$$
由于$\bar{x},x_i$都只和数据$X$有关,我们的目标就可以转为最大化:
$$
\frac{1}{N}\sum_{i=1}^{N}a_i^2
$$
我们之前得到了:$a_i = (x_i-\bar{x})^T\omega$

这表示:
$$
\frac{1}{N}(\sum_{i=1}^{N}a_i^2)\omega = \frac{1}{N}\sum_{i=1}^{N}a_i(x_i-\bar{x})
$$
也就是:
$$
\begin{align*}
\frac{1}{N}\sum_{i=1}^{N}a_i(x_i-\bar{x})&=\frac{1}{N}\sum_{i=1}^N(x_i-\bar{x})a_i\\
&=\frac{1}{N}\sum_{i=1}^{N}(x_i-\bar{x})(x_i-\bar{x})\omega\\
&=Cov(x)\omega
\end{align*}
$$
其中$Cov(x)$是从$X$中计算得到的协方差矩阵。

因此我们知道:
$$
\frac{1}{N}(\sum_{i=1}^{N}a_i^2)\omega =Cov(x)\omega
$$
也就是说:

最优的$\omega$必定是$Cov(x)$的特征向量,并且$\frac{1}{N}(\sum_{i=1}^{N}a_i^2)$是其对应的特征值

由于我们希望最大化$\frac{1}{N}(\sum_{i=1}^{N}a_i^2)$，于是我们可以得出结论：

$\omega$是$Cov(x)$的所有特征向量中拥有最大特征值的那一个。

我们记$Cov(x)$的所有特征向量分别为:$\xi_1,\dots,\xi_D$,按照对应的特征值的大小降序排列。

根据之前得到的关系：$a_i^*=\xi_1^T(x_i-\bar{x})$

从而$x\approx\bar{x}+\xi_1^T(x-\bar{x})\xi_1$

## 更一般的推广

我们猜想,对于更高维度的子空间,都可以用$Cov(x)$的特征向量与特征值来描述。

这是正确的。

记$Cov(x)$的所有特征向量分别为:$\xi_1,\dots,\xi_D$,按照对应的特征值的大小降序排列的特征值分别为:$\lambda_1,\lambda_2,\dots,\lambda_D$,且由于协方差矩阵是半正定的,所有特征值都不小于0,并且所有特征向量的模长都是1.

我们仿照一维的情况可以写出:
$$
x \approx \bar{x}+\xi_1^T(x-\bar{x})\xi_1+\xi_2^T(x-\bar{x})\xi_2+\dots+\xi_D^T(x-\bar{x})\xi_D
$$
如果用新的向量$y$来表示$x$的近似的话,可以写成:
$$
y = (\xi_1^T(x-\bar{x}),\xi_2^T(x-\bar{x}),\dots,\xi_D^T(x-\bar{x}))
$$
每一个特征向量对应的方向称为主成分。

## 方差的分析

首先计算一下$y$的第i个维度上的方差.

首先计算期望$E[y_i]$
$$
E[y_i] = E[\xi_i^T(x-\bar{x})]=\xi_iE[x-\bar{x}]=0
$$
接着计算$Var[y_i]$.
$$
\begin{align*}
Var[y_i]&=E[y_i^2]-E[y_i]^2\\
&=E[y_i^2]\\
&=E[\xi_i^T(x-\bar{x})(x-\bar{x})\xi_i]\\
&=\xi_i^TCov(x)\xi_i\\
&=\lambda_i
\end{align*}
$$
因此$y_i$的均值为0,方差为$\lambda_i$.

还有一个有意思的结果是这样的:
$$
J(\xi_1,\xi_2,\dots,\xi_D)=\frac{\sum_{i=1}^{N}\Vert x_i-\bar{x}\Vert^2}{N}-\lambda_1-\lambda_2-\dots-\lambda_D
$$
也就是说我们挑选的特征值越大,目标函数就越优。

## 总结

从上面的推导我们可以看出,PCA就是最大化投影方向上的数据之间的方差。

## 参考

机械工业出版社【模式识别】——吴健鑫